{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction  \n\n### Session purpose\nIn this session, we are introducing ensemble models based on the decision tree model -- random forest and \neXtreme Gradient Boosting tree (XGBoost). This is the exercise notebook with incomplete code solutions \nwhere you need to complete it based on the comments in the code section. If you are having trouble to \ncomplete the code, you can check the reference notebook.\n\n### Session contents\nIn this session, we will be covering the following topics:\n\n1. Implementation of Random Forest in R;\n2. Implementation of XGBoost in R;\n\n### About the data set  \nThe data set is actual satellite imagery of our home city of Asheville, taken from Landsat 8, an imaging \nsatellite that was launched in 2013.\n\nCheck out the following links for more information: https://www.usgs.gov/land-resources/nli/landsat/landsat-8?qt-science_support_page_related_con=0#qt-science_support_page_related_con\n\nhttps://landsat.gsfc.nasa.gov/landsat-data-continuity-mission/ \n\nBefore we starts to read in the data and create our first classifier, we need to load libraries that will\nbe used in the notebook. We will heavily rely on [**caret**](https://topepo.github.io/caret/index.html) \nlibrary in R for model training. *caret* is a powerful wrapper package which calls hunders of machine \nlearning packages in R and simplify model training and application process.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"library(caret); library(ranger); library(xgboost)                 # pacakge for ML model training\nlibrary(ggplot2); library(cowplot); library(rattle)               # package for visualization\nlibrary(readr); library(dplyr)                                    # package for data handeling","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1 - Review of the data  \n\nIn this tutorial, we are still using the same dataset that we used during our previous training on land \ncover classification for Asheville area. Instead of going through each step in details, we will just do \na quick review of the data.  \n\nFirst thing first, we will read in the data for this notebook which is included in this Github repository.\nThe data file [*NC_L8_GroundTruth.csv*](https://github.com/geo-yrao/ML4ES-tutorials/blob/master/01-Data/NC_L8_GroundTruth.csv) \ncontains sampled pixels in western North Carolina. The data contains both the multispectral reflectance \nfrom Landsat-8 OLI data and corresponding land cover types from USGS Cropland Data Layer (CDL). We can see\nthe first 10 lines of the data. Our data contains the location (*\"Latitude\",\"Longitude\"*), land cover type\n(*\"Class\"*), and reflectance of six OLI channels (*\"B1\"~\"B6\"*). Let's first check how the data frame looks like."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Here, we read in the data pairs between reflectance (scale factor: 0.0001) and land cover types\nfname <- \"~/00-Data/NC_L8_GroundTruth.csv\"\nAVLData <- read_csv(fname); head(AVLData, 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following table present the information about the six [OLI chanles](https://en.wikipedia.org/wiki/Landsat_8) \nincluded in the data. The reflectance data can provide unique information to charaterize different land cover types.\n\n| Channel No. | Channel Name | Wavelength |\n|-:|-:|:-:|\n|B1|Coastal/Areasol|0.433 – 0.453 μm| \n|B2|Blue|0.450 – 0.515 μm|\n|B3|Green|0.525 – 0.600 μm|\n|B4|Red|0.630 – 0.680 μm|\n|B5|Near Infrared|0.845 – 0.885 μm|\n|B6|Short Wavelength Infrared|1.560 – 1.660 μm|  \n\nIn our data, there are five different land cover types as listed in the table below.  \n\n| Class No. | Land Cover Type |\n|-:|-:|\n|0|Forest| \n|1|Corn|\n|2|Soy|\n|3|Development/Urban|\n|4|Water| \n\nIn this tutorials, we merge the two crop types (i.e., corn and soy) together to form a new class as \"Crop\" \nconsidering that they have quite similar spectral features which might lead to misclassification between them."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Create a new variable for merged_class while preserve the original class label from data\n## Merge corn and soy into crop class (with code = 12) and conver it to a factor\nAVLData$merged_class <- factor(if_else((AVLData$Class == 1 | AVLData$Class == 2), 12, AVLData$Class),\n                               levels = c(0,12,3,4), labels = c(\"Forest\", \"Crop\", \"Urban\", \"Water\"))\n\n# Show the histogram of different land cover types using ggplot2\nAVLData %>% ggplot() + geom_bar(aes(merged_class), color=\"black\", fill=\"forestgreen\") +\n  labs(x=\"Land Cover Type\", y=\"No. of Samples\") + coord_cartesian(ylim=c(0,1150), expand=F) +\n  theme_bw() + theme(text=element_text(size=15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## We are now spliting the data set into training and testing data set\ntrainIndex <- createDataPartition(AVLData$merged_class, p=0.8, list=FALSE, times = 1)\n\n## 80% data is for training while 20% data is for independent testing\ntrainData <- AVLData[trainIndex,]; testData <- AVLData[-trainIndex,]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2 - Implementation of Random Forest  \n\nThe Random Forest model is an evolution of the decision tree model that we learned in [earlier session](https://github.com/geo-yrao/ML4ES-tutorials/tree/master/03-DecisionTree).\nThe random forest is an ensemble learning model by creating a suite of decision trees at training \ntime and outputting the class that is the majority of the classes from each individual tree for \nclassification problem. The fundamental idea of random forest model is that the collective power \nof multiple \"weak\" decision tree models can outperform any individual \"strong\" model. It can address\noverfitting issue comparing to the regular decision tree model.  \n\nYou can find more details describing the basics of random forest from the [wekipedia page](https://en.wikipedia.org/wiki/Random_forest) \nor this [blog post by Will Koehrsen](https://medium.com/@williamkoehrsen/random-forest-simple-explanation-377895a60d2d).  \n\nThere are different flavors of random forest model. We are using the classic random forest model \nwith *caret* package with the tag *\"ranger\"*. As the model gets more complicated, we have more to \nconsider about the model structure, such as, the number of decision trees in the forest (*num.trees*), \nmaximum number of features to be considered as input for each tree (*mtry*), the maximum depth of a \ndecision tree (*max.depth*), and the list goes on. To find out the details of individual hyperparameter,\nthere is no better place than the description page of the [function to implment the model *ranger*](https://www.rdocumentation.org/packages/ranger/versions/0.12.1/topics/ranger).\n\nAlthough there are different hyperparameters to tune, *caret* packge prioritize three of them for *ranger* \nmodel -- *mtry*, *min.node.size*, and *splitrule*, in its built-in grid search. But today, we are focusing\non two hyperparameters that will be consistent between the R notebook and Python notebook -- *num.trees* \nand *max.depth*. So we are doing some leg work by modifying the predefined *ranger* model in *caret* library. "},{"metadata":{"trusted":true},"cell_type":"code","source":"### First, we get the original *ranger* model information from *caret* package  \n### using function getModelInfo()\nrangerDefult <- getModelInfo(model = \"ranger\", regex = FALSE)[[1]]\n\n### the rangerOld provides the detailed information of how we want caret to deal with the model\n### and we can simply modify the component of oldModel$ranger$fit to change the behavior of caret\nprint(\"This is the current 'ranger$fit':\")\nprint(rangerDefult$fit)\n\n### In this part, you can see that caret only specify we will tune *mtry*, *min.node.size*,\n### and *splitrule* for ranger. We can simply change that by adding two more hyperparameters\n### by simply change this fit component\nrangerCustom <- rangerDefult\nrangerCustom$parameters = data.frame(parameter = c(\"mtry\", \"splitrule\", \"min.node.size\", \n                                                   \"num.trees\", \"max.depth\"),\n                                     class = c(\"numeric\", \"character\", \"numeric\",\n                                               \"numeric\", \"numeric\"),\n                                     label = c(\"#Randomly Selected Predictors\",\n                                               \"Splitting Rule\",\n                                               \"Minimal Node Size\",\n                                               \"Number of trees\",\n                                               \"Maximum tree depth\"))\nrangerCustom$fit <- function(x, y, wts, param, lev, last, classProbs, ...) {\n                      if((!is.data.frame(x))||dplyr::is.tbl(x)) x <- as.data.frame(x, stringsAsFactors = TRUE)\n                      x$.outcome <- y\n                      if(!is.null(wts)) {\n                        out <- ranger::ranger(dependent.variable.name = \".outcome\",\n                                              data = x,\n                                              num.trees =  param$num.trees,\n                                              max.depth =  max(param$max.depth, 0), \n                                              mtry = min(param$mtry, ncol(x)),\n                                              min.node.size = param$min.node.size,\n                                              splitrule = as.character(param$splitrule),\n                                              write.forest = TRUE,\n                                              probability = classProbs,\n                                              case.weights = wts,\n                                              ...)\n                      } else {\n                        out <- ranger::ranger(dependent.variable.name = \".outcome\",\n                                              data = x,\n                                              num.trees =  param$num.trees,\n                                              max.depth =  max(param$max.depth, 0), \n                                              mtry = min(param$mtry, ncol(x)),\n                                              min.node.size = param$min.node.size,\n                                              splitrule = as.character(param$splitrule),\n                                              write.forest = TRUE,\n                                              probability = classProbs,\n                                              ...)\n                      }\n                      ## in case the resampling method is \"oob\"\n                      if(!last) out$y <- y\n                      out\n                    }\n\n### So now, we can simply use our newly defined model struture with caret package to \n### implement our hyperparameter tuning using the model tag \"rangerCustom\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This cutomized model definition can be a really useful functionality when you are trying to modify\nexisting model that is compatible with *caret* package or you are developing your own machine learning\nmodel but still want to take advantage of the *caret* package such as preprocessing, grid search, etc.\nMore details and examples of how to customize your model with *caret* package can be found [here](https://topepo.github.io/caret/using-your-own-model-in-train.html).   \n\nNow, we can move on with our tutorial to train our random forest model with the customized model tag\n\"rangerCustom\". But first, let's define the hyperparameter grids we want to search through!"},{"metadata":{"trusted":true},"cell_type":"code","source":"### First, let's define the hyperparameter grid we want to search through using function expand.grid()\n### num.trees = 25, 50, 100, 200\n### max.depth = 5, 10, 50, 0 (0 means unlimited tree depth for ranger)\n### We will fix other hyperparameters as its default value\n### mtry = 5\n### splitrule = \"gini\"\n### min.node.size = 1 \nparaGrid <- expand.grid(\n    mtry = 5,\n    splitrule = \"gini\",\n    min.node.size = 1,\n    num.trees = c(25, 50, 100, 200), \n    max.depth = c(5, 10, 50, 0)\n)\n\n### Now we should have 16 different combinations of hyperparameters to search through\nstr(paraGrid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After defining the hyperparameter grids that we want to search through, we will use a 5-fold cross \nvalidation to tune our random forest model using the training dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"### To specify that we want to do 5-fold CV, we need to use the function trainControl() from *caret*.\nrfCtrl <- trainControl(method=\"cv\", number=5, search=\"grid\")\n\n### Once we defined the rfCtrl, we will pass this information as well as the hyperparameter grids that \n### we already defined to function train() to build our model \nrfClassifier <- train(merged_class ~ B1 + B2 + B3 + B4 + B5 + B6, \n                      data = trainData,\n                      method = rangerCustom, \n                      trControl = rfCtrl, \n                      tuneGrid = paraGrid)\nprint(\"Complete model training for customized ranger model...\")\nprint(rfClassifier)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result of hyperparameter tuning tells us that the optimum model structure is a random forest\nmodel with 100 trees and maximum tree depth of 10. We can visualize the results.  \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"### we can see the average performance of the cross validation via confusion matrix\nconfusionMatrix(rfClassifier)\n\n### We can plot the accuracy as a function of different max.depth and num.trees to assist \n### our selection of the hyperparameter\ntrellis.par.set(caretTheme())  ## here, we set the them for the plot\nplot(rfClassifier) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we have our random forest model and it can be applied to our hold-out testing dataset to evaluate\nthe performance of our model.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"### We can predict the class from our rfClassifier using function predict() by specifying \n### what data we are using via \"newdata = ...\"\npredictedClass <- predict(rfClassifier, newdata = testData)\n\n### Let's calculate the confusion matrix using this predicted class and reference class from\n### our test data\nrfConfusionMatrix <- confusionMatrix(predictedClass, testData$merged_class)\n\nprint(rfConfusionMatrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With random forest, we can easily examine the importance of our features used in the model based on \nthe impurity value. This can be easily implemented b yspecifying the argument of \"importance='impurity'\"\nduring training process since by default *ranger* will not return such information.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"### In this part, we are looking at the variable importance of the random forest model\n### that we created. So we will directly train the model again using the optimum hyperparameter sets\n### without cross validation.\n### num.trees = 100, max.depth = 10, mtry = 5, min.node.size= 1, splitrule= \"gini\"\nparaOpt = data.frame(num.trees=100, max.depth=10, mtry=5, min.node.size=1, splitrule=\"gini\")\ntrCtrl  = trainControl(method=\"none\")\nrfClassifier = train(merged_class ~ B1 + B2 + B3 + B4 + B5 + B6,\n                     data = trainData, trControl = trCtrl,\n                     method = rangerCustom, importance=\"impurity\",\n                     tuneGrid = paraOpt)\nvarImp(rfClassifier)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3 - Implement eXtreme Gradient Boosting (XGBoost)  \n\neXtreme Gradient Boosting (XGBoost) is one of the implementation of gradient boosting model family that \nwas proposed by [Leo Breiman in 1997](https://statistics.berkeley.edu/sites/default/files/tech-reports/486.pdf) \nand [Jerome Friedman in 1999](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf). The fundamental idea of\ngradient boosting is to view it as an optimization problem by reducing the model error through a sequential\n\"weak\" models, usually decision trees. \n\nFrom the simple description, you may already sense the difference between random forest model family and \ngradient boosting model family even though they are all based on decision tree model. The random forest model \ncreates an ensemble of independent decision tree models (parallelly) while the gradient boosting model creats \nan ensemble of stage-wise decision tree models (sequentially). Each stage (decision tree) within the gradient\nboosting model is trying to account for model errors from previous stage.\n\nXGBoost is an open source software package developed by [Tianqi Chen](https://arxiv.org/abs/1603.02754) \nand has been included in all major programing languages (e.g., C++, Java, Python, R, Julia, Perl, etc.)\nIt has been the crown jewel in recent data science competitions because of it efficiency and superior performance \n([see some examples](https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions)).\nThe fundamental idea of XGBoost is to create a scalable, portable, and distributed gradient boosting tool.\nThere are more detailed tutorials on how to use XGBoost on their own [community page.](https://xgboost.readthedocs.io/en/latest/R-package/index.html#tutorials)\n\nFor us, we will implement XGBoost via its iterface with *caret* in R and compre the final model performance with\nthe random forest model we created. The model tag for XGBoost using decision tree is *\"xgbTree\"*. The superior \nperformance of *xgbooost* does come at a price -- with even more hyperparameter to consider, such as learning \nrate (*eta*), maximum depth (*max_depth*), regularization, etc. You can find a complete list of hyperparemeters\nthat can be changed in R in [this documentation](https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst)."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Even though there are many parameters that we can change, but to compare with random forest model\n### we will only perform grid search on two -- number of boosting iterations (equivlent to\n### number of trees in random forest) and maximum depth\n### nrounds = 25, 50, 100, 200\n### max_depth = 5, 10, 50, 0\n### eta = 0.3 (default learning rate)\n### gamma = 0 (default minimum loss reduction required to make a further partition on a leaf node of the tree)\n### colsample_bytree = 1 (default subsample ratio of columns when constructing each tree)\n### min_child_weight = 1 (default minimum sum of instance weight (hessian) needed in a child)\n### subsample = 1 (default subsample ratio of the training instances)\nparaGrid <- expand.grid(nrounds = c(25, 50, 100, 200),\n                        max_depth = c(5, 10, 50),\n                        eta = 0.3,\n                        gamma = 0,\n                        colsample_bytree = 1,\n                        min_child_weight = 1,\n                        subsample = 1)\n\n### We have 15 combinations of hyperparameter grids for our cross validation\nstr(paraGrid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will move on to train our XGBoost tree model using function *caret::train()* and *caret::trainControl()*.  \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"### define the 5-fold cross validation using trainControl() function\nxgbCtrl <- trainControl(method=\"cv\", number=5, search=\"grid\")\n\n### innitiate model training with XGBoost (and we only use one processor for the training)\nxgbClassifier <- train(merged_class ~ B1 + B2 + B3+ B4+ B5 + B6, data = trainData,\n                       method = \"xgbTree\", trControl = xgbCtrl, \n                       nthread=1, tuneGrid = paraGrid) \n\nprint(\"Complete XGBoost model training ...\")\nprint(xgbClassifier)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You might noticed that the trianing for tree-based ensemble models (both random forest and XGBoost) is typically\nlonger than our previous model training process (e.g., kNN, decision tree, logistic regression). This is because \nthese ensemble models usually builds hunderds of models at the back end (hunderds of decision tree models in this \ncase). So it will require more computational resources. So be mindful when you are setting the hyperparameter space\nfor model training.  \n\nNow, with the final model training completed, we can examine the model performance via the confusion matrix and \nvisualze the hyperparameter tuning results.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"### First let's look at the confusion matrix for the XGBoost model\nconfusionMatrix(xgbClassifier)\n\n### We can plot the variations of the overall accuracy along different values of our hyperparameters\ntrellis.par.set(caretTheme())  ## here, we set the them for the plot\nplot(xgbClassifier) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we have our final XGBoost tree model and we can apply it to the testing data and compare with our random\nforest model.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"### We can predict the class from our xgbClassifier using function predict() by specifying \n### what data we are using via \"newdata = ...\"\npredictedClass <- predict(xgbClassifier, newdata = testData)\n\n### Let's calculate the confusion matrix using this predicted class and reference class from\n### our test data\nxgbConfusionMatrix <- confusionMatrix(predictedClass, testData$merged_class)\n\nprint(xgbConfusionMatrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's bring two confusion matrix side-by-side to compare the performance of these two models. "},{"metadata":{"trusted":true},"cell_type":"code","source":"### The original confusion matrix is in the format of a table. We will use ggplot to visulize \n### the confusion matrix as a heatmap, so we will slightly modify the confusion matrix into a \n### data frame\nrfCM.df  <- as.data.frame(rfConfusionMatrix$table) ;  str(rfCM.df)\nxgbCM.df <- as.data.frame(xgbConfusionMatrix$table);  str(xgbCM.df)\n\n### Create the heatmap for random forest confusion matrix\nrfHM <- ggplot(data=rfCM.df, aes(Reference, Prediction, fill=Freq)) +\n  geom_tile() + theme_bw() + coord_equal() +\n  scale_fill_distiller(palette=\"Greens\", direction=1) +\n  guides(fill=F) + # removing legend for `fill`\n  labs(title = \"Random Forest Confusion Matrix\") + # using a title instead\n  geom_text(aes(label=Freq), color=\"black\") # printing values\n\n### Create the heatmap for XGBoosting confusion matrix\nxgbHM <- ggplot(data=xgbCM.df, aes(Reference, Prediction, fill=Freq)) +\n  geom_tile() + theme_bw() + coord_equal() +\n  scale_fill_distiller(palette=\"Greens\", direction=1) +\n  guides(fill=F) + # removing legend for `fill`\n  labs(title = \"XGBoosting Confusion Matrix\") + # using a title instead\n  geom_text(aes(label=Freq), color=\"black\") # printing values\n\ncowplot::plot_grid(rfHM, xgbHM, nrow=1, ncol=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this comparison plot of the confusion matrix, it is hard to find significant differences between \nboth random forest and XGBoost model in this case. This is largely because of the data is highly distinguishable\nbetween these four classes. Both model can achieve similarly good performance. The distinguition will be \nmore evident when we face more challenging tasks in the future sessions.  "},{"metadata":{},"cell_type":"markdown","source":"### Bonus Exercise - Isolating Impact of Hyperparameters\n\nYou have now successfully built both random forest model and XGBoosting model. To simplify the training process,\nwe only selected two hyperparameters to tune in earlier steps. I encourage you to examine the impact of \ndifferent hyperparameters separately. This can help you understand which hyperparameter might be more influential\nto your model performance. You can choose either random forest model or XGBoosting model to do this exercise\nin the code chunk below where I have provided some comments on how to do it.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"### To examine the impact of model hyperparameter separately, you can fix all other hyperparameters\n### but the one that you want to study.\n### For example, learning rate *eta* for XGBoosting can be very important to decide your training \n### efficiency and model performance. You can fix all other hyperparameters and modify learning rate\n### in your parameter grids to see how it may affect your model performance by ploting the cross-\n### validation results like we have done before.\n### You can repeat this process for any hyperparameters that you want to investigate.\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat":4,"nbformat_minor":4}