{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nWelcome to Lesson 11 of the North Carolina Institute for Climate Studies' Machine Learning Course. Finally, \nwe've arrived at neural networks (NN)! As mentioned in the lecture, NN are some of the most powerful and most\ninteresting ML algorithms out there. They happen to form the basis for my personal favorites, recurrent \nneural networks, and are capable of learning far more complex patterns than most of the algorithms that \nwe've encountered thus far.\n\nIn this notebook, we'll be working with the most basic type of NN: the multilayer perceptron. We'll be \nusing the same data set as the past several lessons: the US Climate Reference Network's (USCRN) soil \nmoisture data from its Asheville station.\n\nWe will continue to attack the regression problem that we've worked on the past two lessons: attempting \nto estimate the soil moisture levels from precipitation data.\n\nAbout the data set:\nRefer to the USCRN_Daily_Data_readme.txt file in the repository for a complete description of this dataset."},{"metadata":{},"cell_type":"markdown","source":"## Preprocess the data\n\nWe are using the quality controlled daily station observations from US Climate Reference Network (USCRN). \nIn this notebook, we will use the data of Asheville station with nearly 10 years of data. First, let's read \nin the data from the CSV (comma separated variable) file from [our repository](https://github.com/geo-yrao/ML4ES-tutorials/tree/master/00-Data/USCRN-data)."},{"metadata":{"trusted":true},"cell_type":"code","source":"library( dplyr ); library( readr )  # load the library for reading and handaling data\n## Define the file name of the CSV file\nfname <- \"https://raw.githubusercontent.com/geo-yrao/ML4ES-tutorials/master/00-Data/USCRN-data/USCRN-NC_Asheville_8_SSW_2010-2019.csv\"\n\n## Read in the RAW daily data\nRawData <- read.csv(fname)\n\n## Check the column names of the tabular data\nprint ( colnames(RawData) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this notebook, we focus on the problem of estimating the average soil moisture at 5 cm below the \nsurface (_**SOIL_MOISTURE_5_DAILY**_) using other meteorological variables. To keep the model simple,\nwe just use the daily average (or total) of air temperature, precipitation, solar energy, surface \ntemperature, and relative humidity as the model input. Therefore, we need to simplify our current \ntabular data to only keep necessary variables.  \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"### we only keep part of the variables for the application.\n## In addition to the input variables, we kept date to help us separate the data for training/testing\nSlimData <- RawData %>% dplyr::select (., c(2, 9, 10, 11, 15, 18, 19))\n\n## Change coloum names for simple display purpose\ncolnames(SlimData) <- c(\"Date\", \"T2m\", \"Precip\", \"Solar\", \"Tskin\", \"RH\", \"SM_5cm\")\n\n## Check the first & last 10 rows of the data\nhead(SlimData, 10) \ntail(SlimData, 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In addition, we added one more variable called \"SM_10cm_lead\" which is the soil moisture data\nfrom the previous day."},{"metadata":{"trusted":true},"cell_type":"code","source":"SlimData$SM_5cm_lead <-  lag(SlimData$SM_5cm, n=1)\nstr(SlimData)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are missing values in both the independent variables and dependent variables in the \ncurrent data set. Let's see how many missing values exist in the current data set.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"## Summarize the missing value\nmissingSum <- SlimData %>% \n  select_if(function(x) any(is.na(x))) %>%           ## Check if the column contains missing value\n  summarise_all(funs(sum(is.na(.)/length(.)*100)))   ## if so, then count what percent of the data is missing\n\nprint(\"Percentage of missing values in each variable\")\nmissingSum %>% knitr::kable()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears that there is ~36% of data records has missing value for the soil moisture. To proceed with \nmodel development, we will only keep the complete daily data records in this notebook. In the future, \nwe will introduce how to impute missing values for more complex model development.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"CleanData <- SlimData %>% filter(!is.na(T2m), !is.na(Precip),\n                                 !is.na(Solar), !is.na(Tskin),\n                                 !is.na(RH), !is.na(SM_5cm), \n                                 !is.na(SM_5cm_lead))\n\nstr(CleanData)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we implement model development, we want to check the relationship between our variables.\nWe can examine the correlation between our variables pairwise except the date using function\n[_ggpairs()_](https://www.blopig.com/blog/2019/06/a-brief-introduction-to-ggpairs/)."},{"metadata":{"trusted":true},"cell_type":"code","source":"library(GGally)\nggpairs(CleanData, columns = c(2:8),\n        lower = list(continuous = wrap(\"points\", alpha = 0.2, size=0.2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears that the relative humidity (RH) does not show strong correlation with soil moisture,\nso we will remove it from our future model development.\n\nRight now, the *Date* variable is in the format of integer. We need to transform it into the \nspecific format for datetime in R so we can perform time based filtering for training/testing data \nspliting.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"## Convert data type for LST_DATE to Date in R following the format \"YYYY-MM-DD\"\nCleanData$Date <- as.Date(as.character(CleanData$Date), format=\"%Y%m%d\")\nstr(CleanData)\n\n## You will see the data type for LST_DATE has been changed into \"Date\"\n## with this data type, we can easily filter data by observation date for train/test data spliting\n\n## Let's use the data between 2010 and 2014 (5 years) for training our model\n## then, use the data after 2015 for model evaluation\ntrainData <- CleanData %>% filter(Date <= \"2014-12-31\"); dim(trainData)\ntestData  <- CleanData %>% filter(Date >= \"2015-01-01\"); dim(testData)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have two different data sets for model development (*trainData*) and model evaluation (*testData*)\nseperately.\n\n**Cautionary note**: when we split the data into two based on year, there is the underlying assumption \nthat we believe the *trainData* (2010-2014) comes from the same statistical distribution with the *testData*\n(2015-2019). In other words, the dataset used for model development could mostly represent the scenarios\nthat may appear in the dataset for model evaluation. But if there are future extreme events that is beyond the\nrange of *trainData*, we need to treat the prediction carefully since it could have large uncertainties.  "},{"metadata":{},"cell_type":"markdown","source":"## Multiple Layer Perceptron (MLP)\n\nNow, it's time to actually train the MLP regression model. In this notebook, we are using the powerful\nmachine learning library [Kersa](https://keras.rstudio.com/). **Kersa** is a crucial part of the \n[TensorFlow](https://tensorflow.rstudio.com/) ecosystem originally developed by Google Brain Team. \nIt will be your gateway to future deep learning models. It is a good time to get\nyour feet wet in the fast developing field of deep learning using a more intuitive MLP model.\n\nIf you want to learn more about the structure of MLP, this [Towards Data Science blog post](https://towardsdatascience.com/simple-introduction-to-neural-networks-ac1d7c3d7a2c) provides\na very detailed walk through of the fundamentals of the MLP model and how the model training process\nis performed at the back end. \n\nThe R package **keras** is an high level interface to call core functions from Keras and TensorFlow\nwhich are developed outside R. This type of cross platform packages have made it much easier to take\nadvantage of the raipdly developing machine learning landscape in R. We will still use **caret** as \nthe interface to continue our consistent modeling framework from previous tutorials.  \n\nAgain, let's first check the model information of the MLP model in Keras using *getModelInfo()*. The \nmodel tag of MLP in Keras that we use here is *\"mlpKerasDecay\"*."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlibrary(caret); library(keras)\n\n## Getting model information using getModelInfo()\ngetModelInfo(...)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once we have the model information, we can now define the parameter grids that we want to\nsearch through. For the sake of efficiency, we are jsut changing the value of the activation\nfunction and the L2-regularization in the tutorial.  \n\nLet's try these combination of the hyperparameters:\n\n-- Activation function: ReLU (rectified linear unit), sigmoid, linear, and tanh  \n\n-- L2-regularization (lambda): 0.0001, 0.1, 1 (the larger the value, the heavier penalty for overfitting)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"### In this block, we are defining the hyperparameter grid that we want to search through\n### To reduce the computational load for the tutorial, we only choose two hyperparameter \n### in this notebook - activation function, and L2-regularization\n\npara_grid <- expand.grid(...)   ## Defalut node numbers)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So now we should have a parameter grid of 12 different combinations to search through for the \nbest model structure. We can use the *train()* function in **caret** package for a 5-fold cross\nvalidation for model training.  \n\nFirst of all, we need to scale all data to avoid that some large magnitute variables may dominate \nthe model performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"## we will use preProcess function to do the scaling\n## Also, we are not scaling the date for the data\npreProc <- preProcess(...)\n\n## Now, we apply the preprocessing steps to both training data and testing data\ntrainScaled <- predict(...)\ntestScaled  <- predict(...)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While at this stage, you will be asked to install miniconda in order to implement the Keras since\nit requires the python core for the functions. You can simply type \"Y\" to continue the installation."},{"metadata":{"trusted":true},"cell_type":"code","source":"### To specify that we want to do 5-fold CV, we need to use the function trainControl() from *caret*.\ntrCtrl <- trainControl(...)\n\nlibrary(tensorflow); library(keras)\n### Using train function to train the MLP model\n## target : Soil Moisture at 5 cm SM_5cm\n## input  : T2m, Precip, Solar, Tskin\nMLP <- caret::train(...)\n\n### Now we have our mlp GPR model with the optimized hyperparameter\nMLP","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nWith this multilayer perceptron model by accounting for the \"memory\" from previous day, \nour model achieves the coefficient of determination (R^{2}) of 0.45. Let's see how this model will \nperform on the testing data that we set asside. "},{"metadata":{"trusted":true},"cell_type":"code","source":"### First, we apply the model to the test data by using function predict()\nmlpPredicted <- predict(...)\n\n### Now, we want to calculate the RMSE, R^2, and mean absolute error (MAE) using \n### postResample() function\nmlpTesting <- postResample(...)\n\nmlpTesting","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The GPR model estimation shows a very good performance with R^{2} reaching 0.96 and RMSE of 0.25 for \nthe scaled soil moisture value.  \n\nNow, we can visualize our model performance by ploting the true value and the estimation against the \ndate."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Define the result data frame first for ggplot\nresult <- data.frame(...)\n\n## Note that all values are scaled values in the preProcessing step\nggplot(...)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model estimation does not look very promising since it did not capture the data dynamics in the estimation. \nIf you noticed that we did not include the lagged soil moisture from the previous day as our model input.\nWith the Gaussian Process Regression, we demonstrated that adding an additional variable which is the previous day's \nsoil moisture to improve the model performance by accounting for the soil \"memory\". So here comes the exercise for you.  \n\nCan you build a MLP model by adding the previous day soil moisture data to improve the model performance? "},{"metadata":{"trusted":true},"cell_type":"code","source":"\n### You should have all the building blocks for building the model\n### The data is the same, you just need to add an additional input variable to\n### develop the new model.\n\n### Can you compare this model output and the previous model estimation?\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat":4,"nbformat_minor":4}