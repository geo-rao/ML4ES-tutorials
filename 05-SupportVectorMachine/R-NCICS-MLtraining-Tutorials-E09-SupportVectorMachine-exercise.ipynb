{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction  \n\n### Session purpose\nIn this session, we are introducing support vector machine (SVM) for regression applications. It is also \ncalled support vector regression (SVR). Before diving into details about SVR, you should know that SVM \ncan also be used for classification applications with the same procedure of SVR.\n\n### Learning outcome\nAfter going through this tutorial, you will be able to\n\n1. Handle a tabular dataset with missing values for regression applications;\n2. Implement support vector machine for regression applications;\n3. Understand the concept of kernal transformation in SVM.  "},{"metadata":{},"cell_type":"markdown","source":"## 1 - Review the data  "},{"metadata":{"trusted":true},"cell_type":"code","source":"library(kernlab); library(caret)\nlibrary(dplyr); library(magrittr); library(ggplot2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are using the quality controlled daily station observations from US Climate Reference Network (USCRN). \nIn this notebook, we will use the data of Asheville station with nearly 20 years of data. First, let's read \nin the data from the CSV (comma separated variable) file from [our repository](https://github.com/geo-yrao/ML4ES-tutorials/tree/master/00-Data/USCRN-data)."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Define the file name of the CSV file\nfname <- \"../00-Data/USCRN-data/USCRN-NC_Asheville_8_SSW_2001-2019.csv\"\n\n## Read in the RAW daily data\nRawData <- read.csv(fname)\n\n## Check the column names of the tabular data\nprint ( colnames(RawData) )","execution_count":2,"outputs":[{"output_type":"stream","text":" [1] \"WBANNO\"                  \"LST_DATE\"               \n [3] \"CRX_VN\"                  \"LONGITUDE\"              \n [5] \"LATITUDE\"                \"T_DAILY_MAX\"            \n [7] \"T_DAILY_MIN\"             \"T_DAILY_MEAN\"           \n [9] \"T_DAILY_AVG\"             \"P_DAILY_CALC\"           \n[11] \"SOLARAD_DAILY\"           \"SUR_TEMP_DAILY_TYPE\"    \n[13] \"SUR_TEMP_DAILY_MAX\"      \"SUR_TEMP_DAILY_MIN\"     \n[15] \"SUR_TEMP_DAILY_AVG\"      \"RH_DAILY_MAX\"           \n[17] \"RH_DAILY_MIN\"            \"RH_DAILY_AVG\"           \n[19] \"SOIL_MOISTURE_5_DAILY\"   \"SOIL_MOISTURE_10_DAILY\" \n[21] \"SOIL_MOISTURE_20_DAILY\"  \"SOIL_MOISTURE_50_DAILY\" \n[23] \"SOIL_MOISTURE_100_DAILY\" \"SOIL_TEMP_5_DAILY\"      \n[25] \"SOIL_TEMP_10_DAILY\"      \"SOIL_TEMP_20_DAILY\"     \n[27] \"SOIL_TEMP_50_DAILY\"      \"SOIL_TEMP_100_DAILY\"    \n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"From the column names, we see that there are 28 different variables in this tabular data.\nWhat does each column means? It is all documented in the [readme file](https://github.com/geo-yrao/ML4ES-tutorials/blob/master/00-Data/USCRN-data/USCRN_Daily_Data_readme.txt).\n\n* _**WBANNO**_: The station WBAN number.\n* _**LST_DATE**_: The Local Standard Time (LST) date of the observation (YYYYMMDD).\n* _**CRX_VN**_: The version number of the station datalogger program.\n* _**LONGITUDE**_: Station longitude, using WGS-84 (unit: decimal_degrees_east).\n* _**LATITUDE**_: Station latitude, using WGS-84 (unit: decimal_degrees_north).\n* _**T_DAILY_MAX**_: Maximum air temperature (unit: Celsius).\n* _**T_DAILY_MIN**_: Minimum air temperature (unit: Celsius).\n* _**T_DAILY_MEAN**_: Mean air temperature calculated using maximum and minimum temperature (unit: Celsius).\n* _**T_DAILY_AVG**_: Average air temperature calculated using sub-hourly temperature (unit: Celsius).\n* _**P_DAILY_CALC**_: Total amount of precipitation (unit: mm).\n* _**SOLARAD_DAILY**_: Total solar energy (unit: MJ/m^2^).\n* _**SUR_TEMP_DAILY_TYPE**_: Type of infrared surface temperature measurement.\n* _**SUR_TEMP_DAILY_MAX**_: Maximum infrared surface temperature(unit: Celsius).\n* _**SUR_TEMP_DAILY_MIN**_: Minimum infrared surface temperature (unit: Celsius).\n* _**SUR_TEMP_DAILY_AVG**_: Average infrared surface temperature (unit: Celsius).\n* _**RH_DAILY_MAX**_: Maximum relative humidity (unit: %).\n* _**RH_DAILY_MIN**_: Minimum relative humidity (unit: %).\n* _**RH_DAILY_AVG**_: Average relative humidity (unit: %).\n* _**SOIL_MOISTURE_5_DAILY**_: Average soil moisture at 5 cm below the surface (unit: m^3^/m^3^).\n* _**SOIL_MOISTURE_10_DAILY**_: Average soil moisture at 10 cm below the surface (unit: m^3^/m^3^).\n* _**SOIL_MOISTURE_20_DAILY**_: Average soil moisture at 20 cm below the surface (unit: m^3^/m^3^).\n* _**SOIL_MOISTURE_50_DAILY**_: Average soil moisture at 50 cm below the surface (unit: m^3^/m^3^).\n* _**SOIL_MOISTURE_100_DAILY**_: Average soil moisture at 100 cm below the surface (unit: m^3^/m^3^).\n* _**SOIL_TEMP_5_DAILY**_: Average soil temperature at 5 cm below the surface (unit: Celsius).\n* _**SOIL_TEMP_10_DAILY**_: Average soil temperature at 10 cm below the surface (unit: Celsius).\n* _**SOIL_TEMP_20_DAILY**_: Average soil temperature at 20 cm below the surface (unit: Celsius).\n* _**SOIL_TEMP_50_DAILY**_: Average soil temperature at 50 cm below the surface (unit: Celsius).\n* _**SOIL_TEMP_100_DAILY**_: Average soil temperature at 100 cm below the surface (unit: Celsius).\n\nIn this notebook, we focus on the problem of estimating the average soil moisture at 20 cm below the \nsurface (_**SOIL_MOISTURE_20_DAILY**_) using other meteorological variables. To keep the model simple,\nwe just use the daily average (or total) of air temperature, precipitation, solar energy, surface \ntemperature, and relative humidity as the model input. Therefore, we need to simplify our current \ntabular data to only keep necessary variables. "},{"metadata":{"trusted":true},"cell_type":"code","source":"### we only keep part of the variables\nlibrary(dplyr); library(magrittr)\n\n## In addition to the input variables, we kept date to help us separate the data for training/testing\nSlimData <- RawData %>% select ( c(2, 9, 10, 11, 15, 18, 21) )\n\n## Check the first & last 10 rows of the data\nhead(SlimData, 10) \ntail(SlimData, 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are missing values in both the independent variables and dependent variables in the \ncurrent data set. Let's see how many missing values exist in the current data set.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"## Summarize the missing value\nmissingSum <- SlimData %>% \n  select_if(function(x) any(is.na(x))) %>%           ## Check if the column contains missing value\n  summarise_all(funs(sum(is.na(.)/length(.)*100)))   ## if so, then count what percent of the data is missing\n\nmissingSum %>% knitr::kable()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears that there is ~48% of data records has missing value for the soil moisture. To proceed with \nmodel development, we will only keep the complete daily data records in this notebook. In the future, \nwe will introduce how to impute missing values for more complex model development. "},{"metadata":{"trusted":true},"cell_type":"code","source":"CleanData <- SlimData %>% filter(!is.na(T_DAILY_AVG), !is.na(P_DAILY_CALC),\n                                 !is.na(SOLARAD_DAILY), !is.na(SUR_TEMP_DAILY_AVG),\n                                 !is.na(RH_DAILY_AVG), !is.na(SOIL_MOISTURE_20_DAILY))\n\nstr(CleanData)","execution_count":3,"outputs":[{"output_type":"error","ename":"ERROR","evalue":"Error in SlimData %>% filter(!is.na(T_DAILY_AVG), !is.na(P_DAILY_CALC), : could not find function \"%>%\"\n","traceback":["Error in SlimData %>% filter(!is.na(T_DAILY_AVG), !is.na(P_DAILY_CALC), : could not find function \"%>%\"\nTraceback:\n"]}]},{"metadata":{},"cell_type":"markdown","source":"Right now, the *LST_DATE* variable is in the format of integer. We need to transform it into the \nspecific format for datetime in R so we can perform time based filtering for training/testing data \nspliting.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"## Convert data type for LST_DATE to Date in R following the format \"YYYY-MM-DD\"\nCleanData$LST_DATE <- as.Date(as.character(CleanData$LST_DATE), format=\"%Y%m%d\")\nstr(CleanData)\n\n## You will see the data type for LST_DATE has been changed into \"Date\"\n## with this data type, we can easily filter data by observation date for train/test data spliting\n\n## Let's use the data between 2010 and 2017 (8 years) for training our model\n## then, use the data of 2018 and 2019 for model evaluation\ntrainData <- CleanData %>% filter(LST_DATE <= \"2017-12-31\"); dim(trainData)\ntestData  <- CleanData %>% filter(LST_DATE >= \"2018-01-01\"); dim(testData)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have two different data sets for model development (*trainData*) and model evaluation (*testData*)\nseperately.\n\n**Cautionary note**: when we split the data into two based on year, there is the underlying assumption \nthat we believe the *trainData* (2010-2017) comes from the same statistical distribution with the *testData*\n(2018-2019). In other words, the dataset used for model development could mostly represent the scenarios\nthat may appear in the dataset for model evaluation. But if there are future extreme events that is beyond the\nrange of *trainData*, we need to treat the prediction carefully since it could have large uncertainties. "},{"metadata":{},"cell_type":"markdown","source":"## 2 - Building a support verctor regression model  \n\nThe [support vector machine](https://en.wikipedia.org/wiki/Support_vector_machine) was firstly developed in \nlate 1960s and fully implemented by the Bell Laboratory in 1990s for classification tasks. It has been later\nused for regression tasks as well. For regression applications, it is also referred as support vector \nregression (SVR).\n\nThe core idea of SVM is to find a hyperplane or a set of hyperplan to best separate different classes in the\nfeature space when it was firstly developed for classification. All the boundry points of a class in the \nfeature space is called *support vectors*. You can think the hyperplanes as the equalized position when the \nsupport vectors from two different classes are pushing each other in the feature space.  \n\nWhen SVM was applied to regression (SVR), the core principle still applies. But instead of finding the \nhyperplanes (or boundaries) to separate data points in SVM, SVR is identifying the boundaries to include \nas many data points as possible with tolerable error.  \n\nIn R, the SVM/SVR can be implemented using the library **kernlab**. We will use the combination of **caret**\nand **kernlab** in this tutorial to implement SVR for our USCRN data.  \n\nTo start off, we will build a linear SVR model which do not use the kernal function to account for the \nnonlinearity in the data. The model tag for this linear SVR model for **caret** is *\"svmLinear*.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"getModelInfo(...)[[1]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This linear kernal based SVR model has one hyperparameter to tune *\"C\"* which determins the cost\nduring the optimization process. "},{"metadata":{"trusted":true},"cell_type":"code","source":"## First, define model training control & grid for our hyperparameter training\nparaGrid <- expand.grid(\n    ...\n)\n\n### To specify that we want to do 5-fold CV, we need to use the function trainControl() from *caret*.\nlinCtrl <- trainControl(...)\n\n### So now, we are training our linear SVR model using a 5-fold cross validation by searching through \n### six different cost hyperparameters.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the defined hyperparameter grid and training configuration (5-fold cross validation), we can now \nmove forward to train our SVR model. Since we have quite different data range for different input\nvariables, we will also use pre-processing functionality in caret to reduce the impact.  \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Using train function to train the linear SVR model\n## target : SOIL_MOISTURE_20_DAILY\n## input  : T_DAILY_AVG, P_DAILY_CALC, SOLARAD_DAILY, SUR_TEMP_DAILY_AVG, RH_DAILY_AVG\nlinSVR <- train(...)\n\n### Now we have our linear SVR model\nlinSVR","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this first attempt of the linear support vector regression, our training results is not that\nimpressive with the coefficient of determination (R^{2}) of 0.29. Let's see how this model will \nperform on the testing data that we set asside. "},{"metadata":{"trusted":true},"cell_type":"code","source":"### First, we apply the linSVR model to the test data by using function predict()\nlinPredicted <- predict(...)\n\n### Now, we want to calculate the RMSE, R^2, and mean absolute error (MAE) using \n### postResample() function\nlinTesting <- postResample(...)\n\nlinTesting","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The testing results are similar from the model training outcome. Let's take one step further to\nvisualize the outcome of model testing using the scatter plot.  \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Generating a scatter plot between the prediction and observation\nplot(...)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like there are certain nonlinear pattern in the observed data that was not captured by\nour linear SVM model. As noted earlier, SVM can take advantage of the kernal function to transfer\ndata into hyper-space to better charaterize the nonlinearity. So, we will move on to the nonlinear\nkernal for SVM."},{"metadata":{},"cell_type":"markdown","source":"## 3 - Developing a kernal function based SVM  \n\nThere are a variaty of kernal functions that are commonly used for SVM models, such as polinomial\nand radial basis function (RBF). You can find more kernal functions that library **kernlab**\nsupports in this [help document](https://www.rdocumentation.org/packages/kernlab/versions/0.9-29/topics/ksvm).  \n\nHere, we will try both the polinomial function and RBF kernals with **kernlab** and **caret**. In **caret**,\nSVM with RBF kernal is assigned the model tag *\"svmRadial\"* and SVM with polinomial kernal is assigned\nthe model tag *\"svmPoly\"*. You can check more information about the model using the *getModelInfo()* function\nwith corresponding model tags.\n\n### 3.1 - SVM with polynomial kernal  \n\nIn *\"svmPoly\"*, we will need to optimize three model hyperparameters - *C* of the cost by exceeding the\nconstraint, *degree* of the polynomial kernal, and \"scale\" for the polunomial kernal. To the training \nsimple and less computationally demanding, we will keep *C* and *scale* at its default value and\nonly changing *degree* since it tends to have the highest impact. So now let's set up our configuration \nfor model training.  \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## First, define model training control & grid for our hyperparameter training\nparaGrid <- expand.grid(\n    ...\n)\n\n### To specify that we want to do 5-fold CV, we need to use the function trainControl() from *caret*.\npolyCtrl <- trainControl(...)\n\n### So now, we are training our linear SVR model using a 5-fold cross validation by searching through \n### three different degree hyperparameters.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the configuration of hyperparameter grids and 5-fold cross validation, we can now train the \nSVR model with polynomial kernal."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Using train function to train the linear SVR model\n## target : SOIL_MOISTURE_20_DAILY\n## input  : T_DAILY_AVG, P_DAILY_CALC, SOLARAD_DAILY, SUR_TEMP_DAILY_AVG, RH_DAILY_AVG\npolySVR <- train(...)\n\n### Now we have our linear SVR model\npolySVR","execution_count":4,"outputs":[{"output_type":"error","ename":"ERROR","evalue":"Error in train(...): could not find function \"train\"\n","traceback":["Error in train(...): could not find function \"train\"\nTraceback:\n"]}]},{"metadata":{},"cell_type":"markdown","source":"From the 5-fold cross validation results, we did see the model performs better in the training data \nwhen comparing with the linear SVM model. But we can only confidently say that after evaluating the\nmodel against the testing data.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"### First, we apply the linSVR model to the test data by using function predict()\npolyPredicted <- predict(...)\n\n### Now, we want to calculate the RMSE, R^2, and mean absolute error (MAE) using \n### postResample() function\npolyTesting <- postResample(...)\n\npolyTesting","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this evaluation, we see that the SVR model with the polynomial kernal actually performs\nworth than the linear SVR model. This gives an indication of the overfitting in the model\ntraining process since it selected the highest degree for the polynomial kernal which may\nhave lead to model overtly customized to capture the training data pattern. \n\n### 3.2 - SVM with radial basis function kernal \n\nIn *\"svmRadial\"*, we will need to optimize three model hyperparameters - *C* of the cost by exceeding the\nconstraint, *sigma* of the nverse kernel width for radial basis function kernal. To the training \nsimple and less computationally demanding, we will keep *C* at its default value and only changing \n*sigma* since it tends to have the highest impact. So now let's set up our configuration for training.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"## First, define model training control & grid for our hyperparameter training\nparaGrid <- expand.grid(\n    ...\n)\n\n### To specify that we want to do 5-fold CV, we need to use the function trainControl() from *caret*.\nrbfCtrl <- trainControl(...)\n\n### So now, we are training our linear SVR model using a 5-fold cross validation by searching through \n### three different degree hyperparameters.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the configuration of hyperparameter grids and 5-fold cross validation, we can now train the \nSVR model with RBF kernal."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Using train function to train the linear SVR model\n## target : SOIL_MOISTURE_20_DAILY\n## input  : T_DAILY_AVG, P_DAILY_CALC, SOLARAD_DAILY, SUR_TEMP_DAILY_AVG, RH_DAILY_AVG\nrbfSVR <- train(...)\n\n### Now we have our linear SVR model\nrbfSVR","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the 5-fold cross validation results, we did see the model performs better in the training data \nwhen comparing with the linear SVM model. But we can only confidently say that after evaluating the\nmodel against the testing data.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"### First, we apply the linSVR model to the test data by using function predict()\nrbfPredicted <- predict(...)\n\n### Now, we want to calculate the RMSE, R^2, and mean absolute error (MAE) using \n### postResample() function\nrbfTesting <- postResample(...)\n\nrbfTesting","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we can compare the model outcome from three SVM models with different kernals - linear, \npolynomial, and radial basis function."},{"metadata":{"trusted":true},"cell_type":"code","source":"### combine the metrics together in a data frame\ntestingMetric <- data.frame(...)\n\ntestingMetric %>% knitr::kable()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this comparison metric, we can see that using nonlinear kernal function did not improve the \nmodel performance by our intention to capture the data nonlinearity. What may be the reason for\nthis? One question we need to ask is that do we use the best variable for this model to estimate \nsoil moisture data? Also, do we need to consider other information such as the temporal auto-\ncorrelation because soil has memory. We will improve the model following this thought in the future\nnotebooks.\n\n## Extra exercise  \n\nAs we mentioned in previous training, most of the machine leanring models that we introduced before\ncan also be used for regression applications. Now, can you apply a model from previous trainings,\nsuch as XGBoost or random forest to the USCRN data? This is a bonus exercise if you finish the contents\nabove.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"#### We have all the data ready and the formula ready\n#### The only thing you need to think about and adjust is the library that you need for different models\n#### as well as what hyperparameters that you want to tune.\n\n#### Can you implement a xgboost model here?\n\n\n#### How about a random forest model?\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat":4,"nbformat_minor":4}