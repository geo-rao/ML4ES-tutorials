---
title: "NCICS Machine Learning Tutorial - 20200708"
output:
  html_notebook:
    toc: yes
  html_document:
    df_print: paged
    toc: yes
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. You can execute each chunk of code by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*.

## Step 0 - Initialization  
Before we starts to read in the data and create our first classifier, we need to load libraries that will be used in the notebook. We will heavily rely on [**caret**](https://topepo.github.io/caret/index.html) library in R for model training. *caret* is a powerful wrapper package which calls hunders of machine learning packages in R and simplify model training and application process.  
```{r}
library(caret);                                                   # pacakge for ML model training
library(ggplot2); library(cowplot)                                # package for visualization
library(readr); library(dplyr); library(tidyr); library(magrittr) # package for data handeling
```

## Step 1 - Data ingest 

First thing first, we will read in the data for this notebook which is included in this Github repository. The data file [*NC_L8_GroundTruth.csv*](https://github.com/geo-yrao/ML4ES-tutorials/blob/master/NC_L8_GroundTruth.csv) contains sampled pixels in western North Carolina. The data contains both the multispectral reflectance from Landsat-8 OLI data and corresponding land cover types from USGS Cropland Data Layer (CDL).
```{r, message= FALSE, warning = FALSE}
## Here, we read in the data pairs between reflectance (scale factor: 0.0001) and land cover types
fname <- "~/NC_L8_GroundTruth.csv"
AVLData <- read_csv(fname)
```

## Step 2 - View data

Our data contains the location (*"Latitude","Longitude"*), land cover type (*"Class"*), and reflectance of six OLI channels (*"B1"~"B6"*). Let's first check how the data frame looks like.
```{r, message = FALSE}
## We can show the first 10 rows of the data frame 
head(AVLData, 10)
```

The following table present the information about the six [OLI chanles](https://en.wikipedia.org/wiki/Landsat_8) included in the data. The reflectance data can provide unique information to charaterize different land cover types.

| Channel No. | Channel Name | Wavelength |
|-:|-:|:-:|
|B1|Coastal/Areasol|0.433 – 0.453 μm| 
|B2|Blue|0.450 – 0.515 μm|
|B3|Green|0.525 – 0.600 μm|
|B4|Red|0.630 – 0.680 μm|
|B5|Near Infrared|0.845 – 0.885 μm|
|B6|Short Wavelength Infrared|1.560 – 1.660 μm|  

## Step 3 - Viewing class historgram  

We want to understand how many samples belong to each land cover types. In our data, there are five different land cover types as listed in the table below.  

| Class No. | Land Cover Type |
|-:|-:|
|0|Forest| 
|1|Corn|
|2|Soy|
|3|Development/Urban|
|4|Water| 

Here, we create a histogram to examing the histogram of land cover types in the data.  
```{r, warning=FALSE}
# Show the histogram of different land cover types using ggplot2
AVLData %>% ggplot() + geom_histogram(aes(Class), binwidth = 1, color="black", fill="forestgreen") +
  labs(x="Land Cover Type", y="No. of Samples") + coord_cartesian(ylim=c(0,550), xlim=c(-0.75,4.75), expand=F) +
  theme_bw() + theme(text=element_text(size=15))
```

As you can see, the data is overall well balanced across different land cover types except for forest (*Class == 0*). 

## Step 4 - Simpilying to a binary example  

As our first classifier, let's focus on a binary classifier example. So we only keep samples of forests (*Class == 0*) and water (*Class == 4*).
```{r, message=FALSE}
## We use filter function from dplyr to screen the data
binData <- AVLData %>% filter(Class == 0 | Class == 4); str(binData);
## To facilitate the classifier development, we convert the data type of Class from integer to factor in R
binData$Class <- factor(binData$Class, levels = c(0,4), labels = c("Forest", "Water"))
```
As we can see, there are only **`r nrow(binData)`** samples for our binary case instead of all **`r nrow(AVLData)`** samples.

## Step 5 - Visualize data in the feature space  

With the binary class example, we can examine the features (i.e., reflectance) that will be used for building the classifier. We use Channel 4 and 6 as an example. Please consider modify the code below to explore different feature combination.  
```{r, warning=FALSE}
## We use ggplot2 again here to plot the scatter plot of B4 and B6
binData %>% ggplot(aes(x=B4,y=B6)) + geom_point(aes(color=Class), pch=21, size=2) + theme_bw() +
  coord_cartesian(xlim=c(0, 1800), ylim=c(0, 3500), expand=F) + 
  theme(text=element_text(size=15))
```

## Step 6 - Data splitting

To build our first machine learning model, we first need to **split the data** into different sets for model development and evaluation (i.e., model development and testing). Let's follow the convention ratio of data seperation -- 80% model development and 20% testing. We will implement cross validation in the training using the model development dataset. 

There are different ways to implement data splitting. In our case, we want to make sure that both classes are well represented in both data sets. Idealy, we want our data set can reflect on the "reality" so we will use stratified spliting.

```{r, fig.width=9, fig.height=4}
## spliting the data based on the outcome -- land cover type; 80% for modeling, 20% for testing.
set.seed(6302)
trainIndex <- createDataPartition(binData$Class, p=0.8, list=FALSE, times = 1)
trainData <- binData[trainIndex,]; testData <- binData[-trainIndex,]

## Compare the relative frequency for both dataset with the original data using ggplot2
p1 <- binData %>% ggplot() + geom_bar(aes(Class, y=..count../nrow(binData)), color="black", fill="forestgreen") +
  labs(x="Land Cover Type", y="Relative Frequency") + scale_y_continuous(lim=c(0,0.6), breaks = seq(0,0.6,0.1), expand=c(0,0)) +
  theme_bw() + theme(text=element_text(size=15)) ## ploting for overall data
p2 <- trainData %>% ggplot() + geom_bar(aes(Class, y=..count../nrow(trainData)), color="black", fill="royalblue") +
  labs(x="Land Cover Type (Training)", y="Relative Frequency") + 
  scale_y_continuous(lim=c(0,0.6), breaks = seq(0,0.6,0.1), expand=c(0,0)) +
  theme_bw() + theme(text=element_text(size=15)) ## ploting for training data
p3 <- testData %>% ggplot() + geom_bar(aes(Class, y=..count../nrow(testData)), color="black", fill="salmon") +
  labs(x="Land Cover Type (Testing)", y="Relative Frequency") + 
  scale_y_continuous(lim=c(0,0.6), breaks = seq(0,0.6,0.1), expand=c(0,0)) +
  theme_bw() + theme(text=element_text(size=15)) ## ploting for testing data

## putting three plots together via cowplot::plot_grid function
cowplot::plot_grid(p1,p2,p3, nrow=1, ncol=3)
```

How would this splitting different from simple random spliting? Let's see the difference!  
```{r, fig.width=9, fig.height=4}
set.seed(1001)
nobs <- nrow(binData)  # this get the total number of samples
randIndex <- sample(1:nobs, nobs*0.8); # First, we generate the random index for simple random sampling 80%:20%
trainRand <- binData[ randIndex,]; testRand <- binData[-randIndex,] ## simple random spliting dataset

## let's see the comparison of the relative frequency again
## Compare the relative frequency data for both dataset with the original data
p1 <- binData %>% ggplot() + geom_bar(aes(Class, y=..count../nrow(binData)), color="black", fill="forestgreen") +
  labs(x="Land Cover Type", y="Relative Frequency") + scale_y_continuous(lim=c(0,0.6), breaks = seq(0,0.6,0.1), expand=c(0,0)) +
  theme_bw() + theme(text=element_text(size=14)) ## ploting for overall data
p2 <- trainRand %>% ggplot() + geom_bar(aes(Class, y=..count../nrow(trainRand)), color="black", fill="royalblue") +
  labs(x="Land Cover Type (trainRand)", y="Relative Frequency") + scale_y_continuous(lim=c(0,0.6), breaks = seq(0,0.6,0.1), expand=c(0,0)) +
  theme_bw() + theme(text=element_text(size=14)) ## ploting for training data
p3 <- testRand %>% ggplot() + geom_bar(aes(Class, y=..count../nrow(testRand)), color="black", fill="salmon") +
  labs(x="Land Cover Type (testRand)", y="Relative Frequency") + scale_y_continuous(lim=c(0,0.6), breaks = seq(0,0.6,0.1), expand=c(0,0)) +
  theme_bw() + theme(text=element_text(size=14)) ## ploting for testing data

## putting three plots together via cowplot
cowplot::plot_grid(p1,p2,p3, nrow=1, ncol=3)
```
We can see the difference on the relative frequency in our dataset, which is something that we want to avoid when we are handling our dataset. In extreme cases, the imbalance sample could lead to unreliable model training and/or testing results.

## Step 7 - K-nearest neighbor (k-NN)

Now we have training and testing data ready, let's creat our first classifier, [**k-nearest neighbor (k-NN)**](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). There are different variations of k-NN model which are developed to improve its performance for different challenging tasks. But here we will use the classic k-NN model. In *caret* package, the k-NN model is represented using the tag *"knn"*. In R, you can simply get the model information using the function *getModelInfo* provided by *caret* package.
```{r, warning=FALSE}
## check how to use function getModelInfo
? getModelInfo
## check the specifics of the model k-NN. Since there are many different variations of k-nn, we want to match the model tag exactly by turning off the regular expression match.
getModelInfo(model="knn",regex=FALSE)
```
This model information tells us that there is only one model hyperparameter need to be specified to train a k-NN model. To start with the model, we will simply use **k=10** for our first k-NN model without any cross-validation. In *caret*, the function **trainControl** compiles the model training process.
```{r, message=FALSE}
set.seed(998)
## To start our model, we will fix k = 10 without cross validation

## trainControl function in R sepecifies whether and how to train your model
LogControl <- trainControl(method = "none", classProbs = TRUE) 

## using train function to train the model
knnClassifier <- train(Class ~ B1 + B2 + B3 + B4 + B5 + B6, data = trainData, method = "knn", 
                 trControl = LogControl, tuneGrid = data.frame(k=10))

knnClassifier
```

## Step 8 - Generate confusion matrix  

Just like that, we have our first k-NN classifier. How did our model do? We can find out using the confusion matrix.  
```{r, warning=FALSE}
## First, we can extract the predicted class of our k-nn (k=10) model
predClass <- predict(knnClassifier)
## Now, we can combine the predicted class and true class in the trainData to construct our confusion matrix
cfMatrix <- confusionMatrix(predClass, trainData$Class)
cfMatrix
```

## Step 9 - Evaluate model using testing data  

From the confusion matrix, it looks like that our first k-NN model did a decent job on separating these two classes with an accuracy of `r sprintf("%6.4f", cfMatrix$overall[1])`. But how would this model perform on the independent testing data that we set aside at the begining?  
```{r, warning=FALSE}
## Applying our k-NN model to our testData & generate confusion matrix for testing data
knnTest <- predict(knnClassifier, newdata = testData)
cfMatrixTest <- confusionMatrix(knnTest, testData$Class)
cfMatrixTest
```
It looks like the testing performance of our k-NN model is also very promissing with high accuracy (0.9257), which is not significantly different from 

## Step 10 - Cross-validation & Grid Search

It is unlikely that our first guess of k will yield the best model. That's when we need to use cross-validation for hyperparameter (i.e., *k* for k-NN) tuning. To do so, we just need to modify our *trainControl* function. There are many different tuning methods that can be specified in [*trainControl*](https://topepo.github.io/caret/model-training-and-tuning.html#model-training-and-parameter-tuning). The basic cross-validation is specified as `"cv"` by __*caret*__ library. Also, you need to determine how many folds of cross-validation that you want to perform, which is specified as `"number"` in the *trainControl* function. To search the best *k* for our model, let's explore the range from 2 to 15 with the increment of 1.

```{r, warning=FALSE}
## here, we modify the trainControl function to use 5-fold cross-validation 
cvControl <- trainControl(method="cv", number = 5)

## we also need to specify the grid we want the hyperparameter tuning to search
cvGrid <- data.frame(k = seq(1,15,1)); str(cvGrid)

set.seed(982)
## Now, we can implement the cross-validation using train function
knnCV <- train(Class ~ B1 + B2 + B3 + B4 + B5 + B6, data = trainData, method = "knn", 
               trControl = cvControl, tuneGrid = cvGrid)

## Here is how our the cross-validation output looks like
knnCV
```

## Step 11 - Visualize CV results  

Based on the cross-validation result, the *train* function decide to choose 8 as the optimum value for the k-nn because it yields the highest accuracy. But let's visualize it using accuracy as the metric. 

```{r, message=FALSE}
## We can plot the accuracy as a function of different k values to assist our selection of the hyperparameter
trellis.par.set(caretTheme())  ## here, we set the them for the plot
plot(knnCV) 
```

In this plot, we can see the accuracy of our k-NN steadily increases as we increase the number of neighbors (i.e., *k*) before it degradates when k goes beyond 8.  

## Step 12 - Calculate final confusion matrix  

We now calculates the confusion matrix as we have done before but for our model with optmized *k*.  

```{r, warning=FALSE}
cvMatrix <- confusionMatrix(knnCV)
cvMatrix
```


## Step 13 - Apply and evaluate the final model

Now we can apply this final model to our testing data to see if the optimized k value could improve the model performance.

```{r, warning=FALSE}
## apply the model to our testing data
testOutcome <- predict(knnCV, newdata = testData)

## generate the confusion matrix for the testing data
testMatrix <- confusionMatrix(testOutcome, testData$Class)
testMatrix
```

We could compare the confusion matrix with the one that we generated earlier for our first k-NN model with *k* fixed as 10.  
```{r, warning=FALSE}
## This is our confusion matrix from the first k-NN model
cfMatrixTest
```
To conclude, the optimized k-NN model (k=8) via our cross-validation process yields better outcome than our first k-NN model when they were both evaluated using the independent testing data.  